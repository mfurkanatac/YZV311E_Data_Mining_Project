{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cited from https://github.com/mfurkanatac/MBTI-Detector, which had a very very similar problem to ours\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('train_cleaned.csv')\n",
    "\n",
    "# nan values are present in the dataset\n",
    "df['cleaned-text'].replace('', np.nan, inplace=True)\n",
    "df.dropna(subset=['cleaned-text'], inplace=True)\n",
    "df.dropna(subset=['label'], inplace=True)\n",
    "df['label'] = df['label'].astype(int) # since label is float\n",
    "\n",
    "# Split the data into training and validation datasets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['cleaned-text'], df['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "# We'll further split the validation set into validation and test datasets\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model tokenizer \n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode the text into tokens, masks, and segment flags\n",
    "train_encodings = tokenizer(train_text.tolist(), truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(val_text.tolist(), truncation=True, padding=True, max_length=256)\n",
    "test_encodings = tokenizer(test_text.tolist(), truncation=True, padding=True, max_length=256)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "validation_inputs = torch.tensor(val_encodings['input_ids'])\n",
    "train_labels = torch.tensor(train_labels.tolist())\n",
    "validation_labels = torch.tensor(val_labels.tolist())\n",
    "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "validation_masks = torch.tensor(val_encodings['attention_mask'])\n",
    "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
    "test_labels = torch.tensor(test_labels.tolist())\n",
    "test_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=16)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Furkan\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2) # spam (0 and 1) two labels\n",
    "model.to(device)\n",
    "\n",
    "# BERT fine-tuning parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5, correct_bias=False)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)*3) # 3 epochs\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/1585, Loss: 0.62\n",
      "Batch: 250/1585, Loss: 0.20\n",
      "Batch: 500/1585, Loss: 0.54\n",
      "Batch: 750/1585, Loss: 0.01\n",
      "Batch: 1000/1585, Loss: 0.01\n",
      "Batch: 1250/1585, Loss: 0.00\n",
      "Batch: 1500/1585, Loss: 0.01\n",
      "Average training loss: 0.17\n",
      "Validation Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  17%|█▋        | 1/6 [13:30<1:07:32, 810.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/1585, Loss: 0.02\n",
      "Batch: 250/1585, Loss: 0.00\n",
      "Batch: 500/1585, Loss: 0.00\n",
      "Batch: 750/1585, Loss: 0.00\n",
      "Batch: 1000/1585, Loss: 0.01\n",
      "Batch: 1250/1585, Loss: 0.27\n",
      "Batch: 1500/1585, Loss: 0.25\n",
      "Average training loss: 0.06\n",
      "Validation Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|███▎      | 2/6 [26:58<53:57, 809.32s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/1585, Loss: 0.00\n",
      "Batch: 250/1585, Loss: 0.00\n",
      "Batch: 500/1585, Loss: 0.00\n",
      "Batch: 750/1585, Loss: 0.00\n",
      "Batch: 1000/1585, Loss: 0.00\n",
      "Batch: 1250/1585, Loss: 0.23\n",
      "Batch: 1500/1585, Loss: 0.00\n",
      "Average training loss: 0.03\n",
      "Validation Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 3/6 [40:28<40:28, 809.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/1585, Loss: 0.00\n",
      "Batch: 250/1585, Loss: 0.00\n",
      "Batch: 500/1585, Loss: 0.00\n",
      "Batch: 750/1585, Loss: 0.27\n",
      "Batch: 1000/1585, Loss: 0.00\n",
      "Batch: 1250/1585, Loss: 0.00\n",
      "Batch: 1500/1585, Loss: 0.00\n",
      "Average training loss: 0.03\n",
      "Validation Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|██████▋   | 4/6 [54:02<27:02, 811.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/1585, Loss: 0.00\n",
      "Batch: 250/1585, Loss: 0.00\n",
      "Batch: 500/1585, Loss: 0.00\n",
      "Batch: 750/1585, Loss: 0.00\n",
      "Batch: 1000/1585, Loss: 0.00\n",
      "Batch: 1250/1585, Loss: 0.00\n",
      "Batch: 1500/1585, Loss: 0.00\n",
      "Average training loss: 0.02\n",
      "Validation Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  83%|████████▎ | 5/6 [1:07:37<13:32, 812.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/1585, Loss: 0.00\n",
      "Batch: 250/1585, Loss: 0.05\n",
      "Batch: 500/1585, Loss: 0.28\n",
      "Batch: 750/1585, Loss: 0.00\n",
      "Batch: 1000/1585, Loss: 0.00\n",
      "Batch: 1250/1585, Loss: 0.01\n",
      "Batch: 1500/1585, Loss: 0.00\n",
      "Average training loss: 0.03\n",
      "Validation Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 6/6 [1:21:13<00:00, 812.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 6\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if step % 250 == 0:\n",
    "            print(\"Batch: {0}/{1}, Loss: {2:.2f}\".format(step, len(train_dataloader), loss.item()))\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    train_loss_set.append(avg_train_loss)\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    torch.save(model.state_dict(), f'checkpoint2_{_}.pt') # Save the model checkpoint\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), 'bert_model2.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9675\n"
     ]
    }
   ],
   "source": [
    "def evaluate_test_set(test_dataloader):\n",
    "    # Put model in evaluation mode to evaluate loss on the test set\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Test Accuracy: {0:.4f}\".format(eval_accuracy/nb_eval_steps))\n",
    "\n",
    "# Call the function to evaluate the test set\n",
    "evaluate_test_set(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
